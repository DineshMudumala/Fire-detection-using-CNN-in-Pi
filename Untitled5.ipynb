{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "180bsQSG4WHcI-hXjSlYh7MheTFZ4pNrj",
      "authorship_tag": "ABX9TyMZXhLK9ApOiHx/Ta2W9+NS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DineshMudumala/Fire-detection-using-CNN-in-Pi/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vARyxUjM7vLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a262c90-27ba-44ee-b01d-9624cafb7201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FxQjdTYyTyyZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import Data Science Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "# Tensorflow Libraries\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,models\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping,ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# System libraries\n",
        "from pathlib import Path\n",
        "import os.path\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tVrJwZPQ_KCA"
      },
      "outputs": [],
      "source": [
        "dataset = \"/content/drive/MyDrive/Dataset/Fire-Detection\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_omeopM_ry_"
      },
      "outputs": [],
      "source": [
        "image_dir = Path(dataset)\n",
        "\n",
        "# Get filepaths and labels\n",
        "filepaths = list(image_dir.glob(r'**/*.JPG')) + list(image_dir.glob(r'**/*.jpg')) + list(image_dir.glob(r'**/*.png'))\n",
        "\n",
        "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
        "\n",
        "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
        "labels = pd.Series(labels, name='Label')\n",
        "\n",
        "# Concatenate filepaths and labels\n",
        "image_df = pd.concat([filepaths, labels], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgPCeCfJOJbP"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(image_df, test_size=0.2, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train_Generator = ImageDataGenerator(rescale=1./255,\n",
        "                                    shear_range=0.3,\n",
        "                                    zoom_range=0.2,\n",
        "                                    brightness_range=[0.2,0.9],\n",
        "                                    rotation_range=30,\n",
        "                                    horizontal_flip=True,\n",
        "                                    vertical_flip=True,\n",
        "                                    fill_mode=\"nearest\",\n",
        "                                    validation_split=0.1)\n",
        "Test_Generator = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "DpLvxTSVVLey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = Train_Generator.flow_from_dataframe(dataframe=train_df,\n",
        "                                                   x_col='Filepath',\n",
        "                                                   y_col='Label',\n",
        "                                                   color_mode=\"rgb\",\n",
        "                                                   class_mode=\"categorical\",\n",
        "                                                   batch_size=32,\n",
        "                                                   subset=\"training\")\n",
        "validation_images = Train_Generator.flow_from_dataframe(dataframe=train_df,\n",
        "                                                   x_col='Filepath',\n",
        "                                                   y_col='Label',\n",
        "                                                   color_mode=\"rgb\",\n",
        "                                                   class_mode=\"categorical\",\n",
        "                                                   batch_size=32,\n",
        "                                                   subset=\"validation\")\n",
        "test_images = Test_Generator.flow_from_dataframe(dataframe=test_df,\n",
        "                                                 x_col='Filepath',\n",
        "                                                 y_col='Label',\n",
        "                                                 color_mode=\"rgb\",\n",
        "                                                 class_mode=\"categorical\",\n",
        "                                                 batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IWc8US1VX8C",
        "outputId": "06ff33ce-f0a1-4aea-aeee-906ab29bece6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 464 validated image filenames belonging to 2 classes.\n",
            "Found 51 validated image filenames belonging to 2 classes.\n",
            "Found 129 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = (320, 320)"
      ],
      "metadata": {
        "id": "PtFDfn7FZLHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING"
      ],
      "metadata": {
        "id": "LMU1cwOJPJaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "model1.trainable = False"
      ],
      "metadata": {
        "id": "59yvgmp-XAVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6a70df-baa2-4746-ed38-cf569fefc27f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2YqJyGiAU-f"
      },
      "outputs": [],
      "source": [
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.Resizing(224,224),\n",
        "  layers.experimental.preprocessing.Rescaling(1./255),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = model1.input\n",
        "x = resize_and_rescale(inputs)\n",
        "\n",
        "x = Dense(256, activation='relu')(model1.output)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "\n",
        "outputs = Dense(2, activation='softmax')(x)\n",
        "\n",
        "mobilenetmodel = Model(inputs=inputs, outputs=outputs)\n",
        "call_backs = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=5,mode=\"min\")\n",
        "mobilenetmodel.compile(\n",
        "    optimizer=Adam(0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = mobilenetmodel.fit(\n",
        "    train_images,\n",
        "    steps_per_epoch=len(train_images),\n",
        "    validation_data=validation_images,\n",
        "    validation_steps=len(validation_images),\n",
        "    epochs=10,\n",
        "    callbacks=call_backs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wHRNB4mXU_U",
        "outputId": "b26e968c-a20d-4b90-acef-6d8b7b81d0c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "15/15 [==============================] - 152s 10s/step - loss: 0.5148 - accuracy: 0.7845 - val_loss: 0.4317 - val_accuracy: 0.8431\n",
            "Epoch 2/10\n",
            "15/15 [==============================] - 44s 3s/step - loss: 0.3991 - accuracy: 0.8341 - val_loss: 0.3592 - val_accuracy: 0.8431\n",
            "Epoch 3/10\n",
            "15/15 [==============================] - 43s 3s/step - loss: 0.3545 - accuracy: 0.8534 - val_loss: 0.3017 - val_accuracy: 0.9020\n",
            "Epoch 4/10\n",
            "15/15 [==============================] - 43s 3s/step - loss: 0.2827 - accuracy: 0.8922 - val_loss: 0.3081 - val_accuracy: 0.9216\n",
            "Epoch 5/10\n",
            "15/15 [==============================] - 43s 3s/step - loss: 0.2448 - accuracy: 0.8987 - val_loss: 0.2724 - val_accuracy: 0.8824\n",
            "Epoch 6/10\n",
            "15/15 [==============================] - 44s 3s/step - loss: 0.2155 - accuracy: 0.9138 - val_loss: 0.2264 - val_accuracy: 0.9020\n",
            "Epoch 7/10\n",
            "15/15 [==============================] - 42s 3s/step - loss: 0.1817 - accuracy: 0.9289 - val_loss: 0.2082 - val_accuracy: 0.9412\n",
            "Epoch 8/10\n",
            "15/15 [==============================] - 44s 3s/step - loss: 0.1827 - accuracy: 0.9353 - val_loss: 0.2257 - val_accuracy: 0.9020\n",
            "Epoch 9/10\n",
            "15/15 [==============================] - 43s 3s/step - loss: 0.1715 - accuracy: 0.9353 - val_loss: 0.2034 - val_accuracy: 0.9216\n",
            "Epoch 10/10\n",
            "15/15 [==============================] - 44s 3s/step - loss: 0.1490 - accuracy: 0.9440 - val_loss: 0.1614 - val_accuracy: 0.9412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = mobilenetmodel.evaluate(test_images, verbose=0)\n",
        "\n",
        "print(\"    Test Loss: {:.5f}\".format(results[0]))\n",
        "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t68udnQle9Gi",
        "outputId": "bb7f58ff-5b1c-465c-f5fa-17a3e42c19e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Test Loss: 0.21167\n",
            "Test Accuracy: 93.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# Specify export directory and use tensorflow to save your_model\n",
        "export_dir = '/content/drive/MyDrive/tf_lite_models'\n",
        "tf.saved_model.save(mobilenetmodel, export_dir=export_dir)"
      ],
      "metadata": {
        "id": "hdeonWb28H72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.applications.EfficientNetB0(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "model2.trainable = False"
      ],
      "metadata": {
        "id": "h7Ll9lhcgTUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f754592-5508-41ec-8ebd-46817efbfbf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Flatten,BatchNormalization"
      ],
      "metadata": {
        "id": "w0CmpwevkHvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = model2.input\n",
        "x = resize_and_rescale(inputs)\n",
        "\n",
        "x = Flatten()(model2.output)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(1024,activation='relu')(x)\n",
        "x = Dropout(0.2)(x)                  \n",
        "outputs = Dense(2,activation='sigmoid')(x) \n",
        "\n",
        "efficinetmodel = Model(inputs=inputs, outputs=outputs)\n",
        "call_backs = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=4,mode=\"min\")\n",
        "efficinetmodel.compile(\n",
        "    optimizer=Adam(0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history2 = efficinetmodel.fit(\n",
        "    train_images,\n",
        "    steps_per_epoch=len(train_images),\n",
        "    validation_data=validation_images,\n",
        "    validation_steps=len(validation_images),\n",
        "    epochs=10,\n",
        "    callbacks=call_backs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ac16b8-1441-45d3-9244-810de652d92d",
        "id": "gPXu8opvhLQI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "15/15 [==============================] - 80s 5s/step - loss: 0.5186 - accuracy: 0.8082 - val_loss: 0.4744 - val_accuracy: 0.8431\n",
            "Epoch 2/10\n",
            "15/15 [==============================] - 65s 4s/step - loss: 0.5065 - accuracy: 0.8319 - val_loss: 0.5045 - val_accuracy: 0.8431\n",
            "Epoch 3/10\n",
            "15/15 [==============================] - 67s 5s/step - loss: 0.5176 - accuracy: 0.7974 - val_loss: 0.5397 - val_accuracy: 0.8431\n",
            "Epoch 4/10\n",
            "15/15 [==============================] - 66s 5s/step - loss: 0.4886 - accuracy: 0.8319 - val_loss: 0.5230 - val_accuracy: 0.8431\n",
            "Epoch 5/10\n",
            "15/15 [==============================] - 68s 5s/step - loss: 0.4811 - accuracy: 0.8297 - val_loss: 0.5728 - val_accuracy: 0.8431\n",
            "Epoch 6/10\n",
            "15/15 [==============================] - 65s 4s/step - loss: 0.4958 - accuracy: 0.8254 - val_loss: 0.5540 - val_accuracy: 0.8431\n",
            "Epoch 7/10\n",
            "15/15 [==============================] - 65s 4s/step - loss: 0.4787 - accuracy: 0.8297 - val_loss: 0.5452 - val_accuracy: 0.8431\n",
            "Epoch 8/10\n",
            "15/15 [==============================] - 63s 4s/step - loss: 0.4847 - accuracy: 0.8319 - val_loss: 0.5928 - val_accuracy: 0.8431\n",
            "Epoch 9/10\n",
            "15/15 [==============================] - 63s 4s/step - loss: 0.4696 - accuracy: 0.8297 - val_loss: 0.6268 - val_accuracy: 0.8431\n",
            "Epoch 10/10\n",
            "15/15 [==============================] - 62s 4s/step - loss: 0.4798 - accuracy: 0.8297 - val_loss: 0.6301 - val_accuracy: 0.8431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "call_backs = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=4,mode=\"min\")"
      ],
      "metadata": {
        "id": "9R1c9ZgcrREu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}